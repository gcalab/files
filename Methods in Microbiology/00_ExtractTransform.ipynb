{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae7d86f",
   "metadata": {},
   "source": [
    "# Australian AA Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5c4f3",
   "metadata": {},
   "source": [
    "## Part 00: Extract and Transform Metadata\n",
    "\n",
    "The following cells manipulate the GISAID source metadata file,\\\n",
    "extracting sequences collected in Austrailian Regions/Territories.\n",
    "\n",
    "The location of the metadata file (`metadata.tsv.xz`) is specified in\\\n",
    "the cell under \"Definitions > Filepaths\".\n",
    "\n",
    "Several transformed files featuring general summaries will be created\\\n",
    "for use in further analyses.\n",
    "\n",
    "##### General Sequence\n",
    "From the metadata, accession ids are categorized by quarter-year periods (Q1, ...)\n",
    "\n",
    "Using the union of all observed substitutions across all sequences as features, \\\n",
    "amino acid substitutions are coded in a binary manner for each sequence.\n",
    "\n",
    "Then, comparing the count of *all* peer sequences in the region-period,\\\n",
    "will provide the incidence of each substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771aa99",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118530f2",
   "metadata": {},
   "source": [
    "#### Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data')\n",
    "\n",
    "gisaid_metadata_fpath = DATA_DIR / 'metadata.tsv.xz'\n",
    "au_metadata_fpath = DATA_DIR / 'au_metadata.tsv.xz'\n",
    "au_reduced_metadata_fpath = DATA_DIR / 'au_reduced_metadata.tsv.xz'\n",
    "\n",
    "SUMMARY_DIR = DATA_DIR / 'summaries'\n",
    "aa_subs_by_seq_fpath = SUMMARY_DIR / 'aa_subs_by_seq.csv.xz'\n",
    "aa_sub_counts_by_region_period_fname = SUMMARY_DIR / 'aa_sub_counts_by_regional_period.csv'\n",
    "seq_counts_fpath = SUMMARY_DIR / 'seq_counts_per_regional_period.csv'\n",
    "\n",
    "# Uncomment to create the directories\n",
    "# SUMMARY_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e690c",
   "metadata": {},
   "source": [
    "#### DataFrame Column and Index Levels and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_levels = [0,1,2,3,4]\n",
    "index_names=['region', 'collection_date', 'virus_code', 'accession_id', 'time_period']\n",
    "short_index_names = ['region', 'time_period']\n",
    "header_levels=[0,1,2,3]\n",
    "header_names = ['gene', 'position', 'aa_ref', 'aa_sub']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ec2a9",
   "metadata": {},
   "source": [
    "### Extract, Clean and Condense Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if au_metadata_fpath.exists():  # load cached data\n",
    "    au_md_df = pd.read_csv(\n",
    "        au_metadata_fpath,\n",
    "        sep='\\t',\n",
    "        **helpers.CleanedMetadataImportConfig.import_args\n",
    "    ).fillna(helpers.CleanedMetadataImportConfig.nafills)\n",
    "else:\n",
    "    md_df = pd.read_csv(\n",
    "        gisaid_metadata_fpath,\n",
    "        sep='\\t', \n",
    "        **helpers.RawMetadataImportConfig.import_args\n",
    "    ).fillna(helpers.RawMetadataImportConfig.nafills)\n",
    "\n",
    "    # Filter for human hosts only\n",
    "    md_df = md_df[md_df.Host == 'Human']\n",
    "\n",
    "    md_df = helpers.format_column_names(md_df)\n",
    "\n",
    "    # Locations must include \"Australia\"\n",
    "    au_md_df = md_df[md_df['location'].str.contains(pat='[A|a]ustralia')]\n",
    "    # Get all \"complete\" Australian human host data \n",
    "    au_md_df = au_md_df[\n",
    "        (au_md_df['is_low_coverage'] != True) &\n",
    "        (au_md_df['is_complete']==True)\n",
    "    ]\n",
    "\n",
    "    au_md_df = au_md_df[[\n",
    "        'accession_id',\n",
    "        'virus_name',\n",
    "        'location',\n",
    "        'collection_date',\n",
    "        'submission_date',\n",
    "        'clade',\n",
    "        'variant',\n",
    "        'aa_substitutions',\n",
    "    ]].sort_values('collection_date')\n",
    "\n",
    "    # Cache\n",
    "    au_md_df.to_csv(\n",
    "        au_metadata_fpath,\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67084e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_md_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027fad7",
   "metadata": {},
   "source": [
    "#### Note on Collection Date vs Submission Date\n",
    "\n",
    "Preferably, all samples would be assigned a quarter based on\\\n",
    "the Collection Date alone.\n",
    "\n",
    "However, some regions have many collection dates which\\\n",
    "are set to January 1st of a given year (e.g., 2020, 2021, 2022)\n",
    "\n",
    "Since it is unlikely that all sequences were collected on Jan. 1st (especially Jan. 1st, 2020),\n",
    "which happens to be the default date-time if only a year is provided on submission,\n",
    "it is assumed that these are information deficient for purposes of defining a specific quarter year.\n",
    "\n",
    "This error was discovered after reviewing a first pass of the data,\n",
    "finding some apparent Jan. 1st collection dates would place variants (such as omicron) \\\n",
    "quarters before initial detection. \n",
    "\n",
    "With the extreme unlikelihood of a previously unknown variant having 10s-to-100s of samples \n",
    "all collected on January 1st without being reported for more than half a year, especially with\\\n",
    "the date also being the default if only a year was provided upon submission, \\\n",
    "it is assumed the submission date was reasonably soon after the actual collection.\n",
    "\n",
    "Given the proportion of samples from certain regions which only provided the year,\n",
    "the Submission Date was instead used as a proxy. This comes with a few limitations and caveats:\n",
    "\n",
    "1. The time between the collection and submission occured across a quarter.\n",
    "2. The time between the collection and submission was exceptionally long, i.e., across more than one quarter\n",
    "\n",
    "However, results after the adjustment generally align with known origin dates of variants,\n",
    "and so this heuristic was sufficient for purposes of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20199538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if au_reduced_metadata_fpath.exists():  # load cached data\n",
    "    au_reduced_df = pd.read_csv(\n",
    "        au_reduced_metadata_fpath,\n",
    "        sep='\\t'\n",
    "    )\n",
    "else:\n",
    "    au_reduced_df = au_md_df.copy()\n",
    "\n",
    "    # filter all January 1st dates\n",
    "    jan_first_filter = (\n",
    "        (au_reduced_df.collection_date == pd.to_datetime('2020-01-01')) |\n",
    "        (au_reduced_df.collection_date == pd.to_datetime('2021-01-01')) |\n",
    "        (au_reduced_df.collection_date == pd.to_datetime('2022-01-01'))\n",
    "    )\n",
    "    au_reduced_df.loc[jan_first_filter, 'collection_date'] = au_reduced_df['submission_date']\n",
    "\n",
    "    # Split into year, month, and day columns (day is arbitrary)\n",
    "    au_reduced_df['year'] = au_reduced_df['collection_date'].dt.year\n",
    "    au_reduced_df['month'] = au_reduced_df['collection_date'].dt.month\n",
    "    au_reduced_df['day'] = au_reduced_df['collection_date'].dt.day\n",
    "\n",
    "    # split location string into separate columns\n",
    "    expanded_loc_cols = ['continent', 'country', 'region', 'city']\n",
    "    au_reduced_df[expanded_loc_cols] = au_reduced_df['location'].str.split('/', expand=True)\n",
    "\n",
    "    # Format to succinct \"Virus Code\"\n",
    "    au_reduced_df['virus_code'] = helpers.get_formatted_virus_code(au_reduced_df['virus_name'])\n",
    "\n",
    "    # Create columns for deliniating region-periods\n",
    "    au_reduced_df['quarter'] = au_reduced_df.apply(helpers.get_quarter, axis=1)\n",
    "    au_reduced_df['time_period'] = au_reduced_df.apply(helpers.get_period, axis=1)\n",
    "\n",
    "    au_reduced_df['region'] = au_reduced_df['region'].str.strip()\n",
    "    au_reduced_df['region'] = au_reduced_df['region'].str.replace('?', '', regex=False)\n",
    "\n",
    "    au_reduced_df = au_reduced_df[[\n",
    "        'accession_id',\n",
    "        'virus_name',\n",
    "        'virus_code',\n",
    "        'collection_date',\n",
    "        'submission_date',\n",
    "        'year',\n",
    "        'month',\n",
    "        'day',\n",
    "        'quarter',\n",
    "        'time_period',\n",
    "        # 'continent',  # must be Australia\n",
    "        # 'country',  # must be Australia\n",
    "        'region',\n",
    "        'clade',\n",
    "        'variant',\n",
    "        'aa_substitutions',\n",
    "    ]]\n",
    "\n",
    "    au_reduced_df = au_reduced_df.sort_values('collection_date')\n",
    "\n",
    "    # cache\n",
    "    au_reduced_df.to_csv(\n",
    "        au_reduced_metadata_fpath,\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291da49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_reduced_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d487a",
   "metadata": {},
   "source": [
    "### AA-Substitution Vectorization\n",
    "\n",
    "The following cells build binary vectors for each known substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47124acb",
   "metadata": {},
   "source": [
    "#### Extract the Amino Acid Substitutions (\"AA_Substitutions\" column)\n",
    "\n",
    "Each `aa_substitution` identifier string takes the form:\n",
    "    \n",
    "    {gene}_{reference_aa}{position}{substitute_aa}\n",
    "\n",
    "This is split into the constituent parts to produce a multi-leveled column, ordered as:\n",
    "\n",
    "1. Gene\n",
    "2. Position\n",
    "3. Reference AA\n",
    "4. Substituted AA\n",
    "\n",
    "Note the inversion of the position and reference aa.\n",
    "\n",
    "Then, each row describes the presence (or lack thereof) of the substitution within the sample.\n",
    "\n",
    "For example, `spike_D614G` would be split into a hierarchical column:\n",
    "\n",
    "1. Spike\n",
    "2. 614\n",
    "3. D\n",
    "4. G\n",
    "\n",
    "and would contain a `0` in `EPI_ISL_402125` (the reference sequence), \\\n",
    "but a `1` for delta variant samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if aa_subs_by_seq_fpath.exists():\n",
    "    aa_subs_by_seq_df = pd.read_csv(\n",
    "        aa_subs_by_seq_fpath,\n",
    "        index_col=index_levels,\n",
    "        header=header_levels\n",
    "        # index_names=index_names\n",
    "    ).fillna(0)\n",
    "else:\n",
    "    # iterate through substitutions, remove parentheses, split by comma. \n",
    "    # If substitution exists, mark with `1`\n",
    "    aa_subs = {\n",
    "        idx: pd.Series(\n",
    "            [1 for _ in row[1:-1].split(',')], index=row[1:-1].split(',')\n",
    "        ) for idx, row in au_reduced_df.set_index(index_names)['aa_substitutions'].items()\n",
    "    }\n",
    "    aa_subs_by_seq_df = pd.DataFrame.from_dict(aa_subs, orient='index')\n",
    "    aa_subs_by_seq_df = aa_subs_by_seq_df.fillna(0).sort_index(axis=1).drop(columns='', errors='ignore')\n",
    "    aa_subs_by_seq_df.index = aa_subs_by_seq_df.index.rename(index_names)\n",
    "    aa_subs_by_seq_df = aa_subs_by_seq_df.astype('int32')\n",
    "\n",
    "    # create levels\n",
    "    aa_multi_cols = aa_subs_by_seq_df.columns.str.extractall(r\"(?P<gene>.+?)_(?P<aa_ref>([A-Z]+?)|(ins))(?P<position>[0-9]+)(?P<aa_sub>.+)\")\n",
    "    aa_multi_cols = aa_multi_cols.fillna('').droplevel(1).drop([2,3], axis=1)\n",
    "    aa_multi_cols = aa_multi_cols.set_index(aa_subs_by_seq_df.columns)\n",
    "    aa_multi_cols = aa_multi_cols[['gene', 'position', 'aa_ref', 'aa_sub']]\n",
    "    \n",
    "    # Apply to new dataframe\n",
    "    aa_subs_by_seq_df.columns = pd.MultiIndex.from_tuples(\n",
    "        aa_multi_cols.values.tolist(), \n",
    "        names=aa_multi_cols.columns.tolist()\n",
    "    )\n",
    "\n",
    "    aa_subs_by_seq_df = aa_subs_by_seq_df.sort_index(axis=1).sort_index(axis=0)\n",
    "\n",
    "    if '.xz' not in str(aa_subs_by_seq_fpath):\n",
    "        # create a fake \"sparse\" table to reduce character count\n",
    "        aa_subs_by_seq_df.replace(0, '').to_csv(aa_subs_by_seq_fpath)\n",
    "    else:\n",
    "        aa_subs_by_seq_df.to_csv(aa_subs_by_seq_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cce9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_subs_by_seq_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047c3ec",
   "metadata": {},
   "source": [
    "## Summary Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a117b5",
   "metadata": {},
   "source": [
    "### Substitution Count by each Region-Period\n",
    "\n",
    "Calculate the sum of each substitution column, partitioned by combined region-period  (e.g., New Queensland Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_sub_counts_by_region_period = aa_subs_by_seq_df.groupby(level=['region', 'time_period']).sum()\n",
    "aa_sub_counts_by_region_period.to_csv(aa_sub_counts_by_region_period_fname)\n",
    "\n",
    "# fill missing time periods with \"0\"\n",
    "regions = aa_sub_counts_by_region_period.index.get_level_values(0).unique()\n",
    "max_period = aa_sub_counts_by_region_period.index.get_level_values(1).max()\n",
    "\n",
    "aa_sub_counts_by_region_period = aa_sub_counts_by_region_period.reindex([\n",
    "    (region, period) \n",
    "    for region in regions \n",
    "    for period in range(1,max_period + 1)\n",
    "]).fillna(0).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_sub_counts_by_region_period.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b8731",
   "metadata": {},
   "source": [
    "### Sequence Counts by Regional Period\n",
    "\n",
    "Calculate the number of Australian sequences, partitioned by combined region-period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if seq_counts_fpath.exists():\n",
    "    ## This will take at least 7 seconds to retrieve\n",
    "    seq_counts_df = pd.read_csv(\n",
    "        seq_counts_fpath,\n",
    "        index_col=short_index_names\n",
    "    )\n",
    "else:\n",
    "    seq_counts_df = aa_subs_by_seq_df.droplevel([1,2]).index.to_frame()\n",
    "    \n",
    "    seq_counts_df = seq_counts_df.reset_index(drop=True)\n",
    "    seq_counts_df = seq_counts_df.groupby(short_index_names).count()\n",
    "    seq_counts_df = seq_counts_df.rename(columns={'accession_id': 'counts'})\n",
    "\n",
    "    seq_counts_df.to_csv(seq_counts_fpath)\n",
    "\n",
    "# fill missing time periods with \"0\"\n",
    "regions = seq_counts_df.index.get_level_values(0).unique()\n",
    "max_period = seq_counts_df.index.get_level_values(1).max()\n",
    "\n",
    "seq_counts_df = seq_counts_df.reindex([\n",
    "    (region, period) \n",
    "    for region in regions \n",
    "    for period in range(1,max_period + 1)\n",
    "]).fillna(0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_counts_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc689a",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Find incidence of Spike D614G across region-periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_sub_counts_by_region_period.div(seq_counts_df['counts'].values, axis=0).loc[\n",
    "    :,\n",
    "    aa_sub_counts_by_region_period.columns.get_level_values('position') == '614'\n",
    "].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907795da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Australian Seasonal SARS-CoV2 Mutations",
   "language": "python",
   "name": "covmut _au"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
